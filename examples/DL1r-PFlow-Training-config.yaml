# Set modelname and path to Pflow preprocessing config file
model_name: DL1r-PFlow_new-taggers-stats-22M
preprocess_config: examples/PFlow-Preprocessing.yaml

# Add here a pretrained model to start with.
# Leave empty for a fresh start
model_file:

# Add training file
train_file: /work/ws/nemo/fr_af1100-Training-Simulations-0/preprocessed/PFlow-hybrid-preprocessed_shuffled.h5

# Add validation files
# ttbar val
validation_file: /work/ws/nemo/fr_af1100-Training-Simulations-0/hybrids/MC16d_hybrid_odd_100_PFlow-no_pTcuts-file_0.h5

# zprime val
add_validation_file: /work/ws/nemo/fr_af1100-Training-Simulations-0/hybrids/MC16d_hybrid-ext_odd_0_PFlow-no_pTcuts-file_0.h5

ttbar_test_files:
    ttbar_r21:
        Path: /work/ws/nemo/fr_af1100-Training-Simulations-0/hybrids/MC16d_hybrid_odd_100_PFlow-no_pTcuts-file_1.h5
        data_set_name: "ttbar"

    ttbar_r22:
        Path: /work/ws/nemo/fr_af1100-Training-Simulations-0/hybrids_r22/MC16d_hybrid-r22_odd_100_PFlow-no_pTcuts-file_1.h5
        data_set_name: "ttbar_comparison"

zpext_test_files:
    zpext_r21:
        Path: /work/ws/nemo/fr_af1100-Training-Simulations-0/hybrids/MC16d_hybrid-ext_odd_0_PFlow-no_pTcuts-file_1.h5
        data_set_name: "zpext"

    zpext_r22:
        Path: /work/ws/nemo/fr_af1100-Training-Simulations-0/hybrids_r22/MC16d_hybrid-r22-ext_odd_0_PFlow-no_pTcuts-file_1.h5
        data_set_name: "zpext_comparison"

    zpext_r22_no_QSP:
        Path: /work/ws/nemo/fr_af1100-Training-Simulations-0/hybrids_r22/MC16d_hybrid-r22-ext_odd_0_PFlow-no_pTcuts_No_QSPI-file_1.h5
        data_set_name: "zpext_comparison_no_QSP"

# Path to Variable dict used in preprocessing
var_dict: umami/configs/DL1r_Variables.yaml

bool_use_taus: False

exclude: []

NN_structure:
    lr: 0.01
    batch_size: 15000
    activations: ["relu", "relu", "relu", "relu", "relu", "relu", "relu", "relu"]
    units: [256, 128, 60, 48, 36, 24, 12, 6]

# Eval parameters for validation evaluation while training
Eval_parameters_validation:
    # Number of jets used for validation
    n_jets: 3e5

    # Define taggers that are used for comparison in evaluate_model
    # This can be a list or a string for only one tagger
    tagger: ["rnnip", "DL1r"]

    # Define fc values for the taggers
    fc_values_comp: {
        "rnnip": 0.08,
        "DL1r": 0.018,
    }

    # Define fb values for the taggers
    fb_values_comp: {
        "rnnip": 0.2,
        "DL1r": 0.2,
    }

    # Charm fraction value used for evaluation of the trained model
    fc_value: 0.018

    # B fraction value used for evaluation
    fb_value: 0.2

    # Tau fraction value used for evaluation (for c)
    ftauforc_value: None

    # # Tau fraction value used for evaluation (for b)
    ftauforb_value: None

    # b Working point used in the evaluation
    WP_b: 0.77
    # C Working point used in the evaluation
    WP_c: 0.4

    # Set minimum of accuracy plot y-axis
    acc_ymin: 0.59
    # Set maximum of accuracy plot y-axis
    acc_ymax: 1.0

    # A list to add available variables to the evaluation files
    add_variables_eval: ["actualInteractionsPerCrossing"]

    # Enable/Disable atlas tag
    UseAtlasTag: True

    # fc_value and WP_b are autmoatically added to the plot label
    AtlasTag: "Internal Simulation"
    SecondTag: "\n$\\sqrt{s}=13$ TeV, PFlow jets"

    # Set the datatype of the plots
    plot_datatype: "pdf"

    # some properties for the feature importance explanation with SHAPley
    shapley:
        # Over how many full sets of features it should calculate over.
        # Corresponds to the dots in the beeswarm plot.
        # 200 takes like 10-15 min for DL1r on a 32 core-cpu
        feature_sets: 200

        # defines which of the model outputs (flavor) you want to explain
        # [tau,b,c,u] := [3, 2, 1, 0]
        model_output: 2

        # You can also choose if you want to plot the magnitude of feature
        # importance for all output nodes (flavors) in another plot. This
        # will give you a bar plot of the mean SHAP value magnitudes.
        bool_all_flavor_plot: False

        # as this takes much longer you can average the feature_sets to a
        # smaller set, 50 is a good choice for DL1r
        averaged_sets: 50

        # [11,11] works well for dl1r
        plot_size: [11, 11]
