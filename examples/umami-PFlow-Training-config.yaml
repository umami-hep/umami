# Set modelname and path to Pflow preprocessing config file
model_name: umami_dev
preprocess_config: <path_palce_holder>/PFlow-Preprocessing-DESY.yaml

# Add here a pretrained model to start with.
# Leave empty for a fresh start
model_file:

# Add training file
train_file: <path_palce_holder>/PFlow-hybrid_70-test-preprocessed_shuffled.h5

# Defining templates for the variable cuts
.variable_cuts_ttbar: &variable_cuts_ttbar
    variable_cuts:
        - pt_btagJes:
            operator: "<="
            condition: 2.5e5

.variable_cuts_zpext: &variable_cuts_zpext
    variable_cuts:
        - pt_btagJes:
            operator: ">"
            condition: 2.5e5

# Add validation files
validation_files:
    ttbar_r21_val:
        path: <path_palce_holder>/MC16d_hybrid_odd_100_PFlow-no_pTcuts-file_0.h5
        label: "$t\\bar{t}$ Release 21"
        <<: *variable_cuts_ttbar

    zprime_r21_val:
        path: <path_palce_holder>/MC16d_hybrid-ext_odd_0_PFlow-no_pTcuts-file_0.h5
        label: "$Z'$ Release 21"
        <<: *variable_cuts_zpext

test_files:
    ttbar_r21:
        path: <path_palce_holder>/MC16d_hybrid_odd_100_PFlow-no_pTcuts-file_1.h5
        <<: *variable_cuts_ttbar

    ttbar_r22:
        path: <path_palce_holder>/MC16d_hybrid-r22_odd_100_PFlow-no_pTcuts-file_1.h5
        <<: *variable_cuts_ttbar

    zpext_r21:
        path: <path_palce_holder>/MC16d_hybrid-ext_odd_0_PFlow-no_pTcuts-file_1.h5
        <<: *variable_cuts_zpext

    zpext_r22:
        path: <path_palce_holder>/MC16d_hybrid-r22-ext_odd_0_PFlow-no_pTcuts-file_1.h5
        <<: *variable_cuts_zpext

# Path to Variable dict used in preprocessing
var_dict: <path_palce_holder>/umami/umami-git/umami/configs/Umami_Variables.yaml

exclude: null

# Tracks dataset name
tracks_name: "tracks"

# number of files to be loaded in parallel when using TF Records as input files
nfiles: 5

NN_structure:
    # Decide, which tagger is used
    tagger: "umami"

    # NN Training parameters
    lr: 0.01
    batch_size: 5000
    epochs: 200

    # Number of jets used for training
    # To use all: Fill nothing
    nJets_train:

    # Dropout rate for the DIPS block. If = 0, dropout is disabled
    dropout: 0

    # Decide if Batch Normalisation is used in the DIPS block
    Batch_Normalisation: True

    # DIPS structure
    DIPS_ppm_units: [100, 100, 128]
    DIPS_dense_units: [100, 100, 100, 30]

    # These are the layers that will be concatenated with the last layer of DIPS_dense_uits
    intermediate_units: [72]

    # DL1 structure
    DL1_units: [57, 60, 48, 36, 24, 12, 6]

    # total loss = loss(umami) + dips_loss_weight * loss(dips)
    dips_loss_weight: 1

    # Define which classes are used for training
    # These are defined in the global_config
    class_labels: ["ujets", "cjets", "bjets"]

    # Main class which is to be tagged
    main_class: "bjets"

    # Options for the Learning Rate reducer
    LRR: True

    # Option if you want to use sample weights for training
    use_sample_weights: False

# Plotting settings for training metrics plots
Validation_metrics_settings:
    # Define which taggers should also be plotted
    taggers_from_file: ["rnnip", "DL1r"]

    # Enable/Disable atlas tag
    UseAtlasTag: True

    # fc_value and WP_b are autmoatically added to the plot label
    AtlasTag: "Internal Simulation"
    SecondTag: "\n$\\sqrt{s}=13$ TeV, PFlow jets"

    # Set the datatype of the plots
    plot_datatype: "pdf"

Eval_parameters_validation:
    # Number of jets used for each validation sample. Set this to 0 if you have memory problems
    n_jets: 3e5

    # Define taggers that are used for comparison in evaluate_model
    # This can be a list or a string for only one tagger
    tagger: ["rnnip", "DL1r"]

    # Define fc values for the taggers
    frac_values_comp: {
        "rnnip": {
            "cjets": 0.08,
            "ujets": 0.92,
        },
        "DL1r": {
            "cjets": 0.018,
            "ujets": 0.982,
        },
    }

    # Charm fraction value used for evaluation of the trained model
    frac_values: {
        "dips": {
            "cjets": 0.018,
            "ujets": 0.982,
        },
        "umami": {
            "cjets": 0.018,
            "ujets": 0.982,
        },
    }

    # Working point used in the evaluation
    WP: 0.77
